{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luckagianvechio/Documents/Material Estudo TCC/code/all/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luckagianvechio/Documents/Material Estudo TCC/code/all/lib/python3.9/site-packages/transformers/models/layoutlmv3/processing_layoutlmv3.py:195: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv3Processor\n",
    "\n",
    "lmv3_processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
    "lmv3_processor.feature_extractor.apply_ocr = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3Tokenizer\n",
    "\n",
    "lmv3_tok = LayoutLMv3Tokenizer.from_pretrained(\"microsoft/layoutlmv3-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from os import listdir\n",
    "\n",
    "main_path = Path(\"/home/luckagianvechio/Documents/Material Estudo TCC/IIT CDIP/images.a.a/imagesa/a/a\")\n",
    "a_path = Path(\"a/\")\n",
    "a_img_folder_path = [main_path / a_path / Path(pt) for pt in listdir(main_path / a_path)]\n",
    "a_img_path = []\n",
    "for pt in a_img_folder_path:\n",
    "    files = listdir(pt)\n",
    "    for file in files:\n",
    "        if not file.split(\".\")[1] == \"xml\":\n",
    "            a_img_path.append(pt / file)\n",
    "\n",
    "from ocr_tools import get_ocr_word_box_list, preprocess_image, resize_image, read_image, normalize_bbox\n",
    "\n",
    "image = read_image(a_img_path[0])\n",
    "text_boxes, shape = get_ocr_word_box_list(a_img_path[0])\n",
    "words = [k[\"text\"] for k in text_boxes]\n",
    "boxes = [normalize_bbox(k[\"bbox\"], shape[0], shape[1]) for k in text_boxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = lmv3_processor(\n",
    "    image,\n",
    "    words,\n",
    "    boxes=boxes,\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> \t\t tensor([0, 0, 0, 0])\n",
      " Alt \t\t tensor([13, 12, 37, 30])\n",
      "empt \t\t tensor([13, 12, 37, 30])\n",
      " do \t\t tensor([38, 12, 48, 30])\n",
      "e \t\t tensor([38, 12, 48, 30])\n",
      " em \t\t tensor([84, 15, 97, 16])\n",
      " age \t\t tensor([340,  12, 353,  30])\n",
      "� \t\t tensor([340,  12, 353,  30])\n",
      "� \t\t tensor([340,  12, 353,  30])\n",
      " 7 \t\t tensor([356,  14, 362,  27])\n",
      "} \t\t tensor([356,  14, 362,  27])\n",
      " RJ \t\t tensor([ 98, 126, 113, 135])\n",
      ". \t\t tensor([ 98, 126, 113, 135])\n",
      " RE \t\t tensor([123, 126, 128, 135])\n",
      " ER \t\t tensor([260, 126, 277, 135])\n",
      "T \t\t tensor([260, 126, 277, 135])\n",
      " Alert \t\t tensor([ 71, 154,  86, 161])\n",
      " Date \t\t tensor([ 89, 154, 104, 161])\n",
      ": \t\t tensor([ 89, 154, 104, 161])\n",
      " December \t\t tensor([130, 154, 161, 161])\n"
     ]
    }
   ],
   "source": [
    "for id, box in zip(processed['input_ids'][0][:20], processed['bbox'][0][:20]):\n",
    "    print(lmv3_tok.decode([id]),\"\\t\\t\" ,box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Altempt', 'doe', 'em', 'age’', '7}', 'RJ.', 'RE', 'ERT', 'Alert', 'Date:']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ C L S ]\n",
      "[ U N K ]\n",
      "[ U N K ]\n",
      "e m\n",
      "[ U N K ]\n",
      "[ U N K ]\n",
      "[ U N K ]\n",
      "[ U N K ]\n",
      "[ U N K ]\n",
      "[ U N K ]\n",
      "[ U N K ]\n",
      "[ S E P ]\n"
     ]
    }
   ],
   "source": [
    "for tk in tokenizer.encode(words[:10]): print(tokenizer.decode(tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Al', '##tem', '##pt']\n",
      "[101, 476, 2097, 5131, 102]\n",
      "[CLS] Altempt [SEP]\n",
      "\n",
      "['do', '##e']\n",
      "[101, 171, 22279, 102]\n",
      "[CLS] doe [SEP]\n",
      "\n",
      "['em']\n",
      "[101, 173, 102]\n",
      "[CLS] em [SEP]\n",
      "\n",
      "['age', '’']\n",
      "[101, 9174, 22361, 102]\n",
      "[CLS] age ’ [SEP]\n",
      "\n",
      "['7', '}']\n",
      "[101, 977, 197, 102]\n",
      "[CLS] 7 } [SEP]\n",
      "\n",
      "['RJ', '.']\n",
      "[101, 19647, 119, 102]\n",
      "[CLS] RJ. [SEP]\n",
      "\n",
      "['R', '##E']\n",
      "[101, 257, 22309, 102]\n",
      "[CLS] RE [SEP]\n",
      "\n",
      "['E', '##RT']\n",
      "[101, 192, 20257, 102]\n",
      "[CLS] ERT [SEP]\n",
      "\n",
      "['Ale', '##r', '##t']\n",
      "[101, 1043, 22282, 22286, 102]\n",
      "[CLS] Alert [SEP]\n",
      "\n",
      "['Da', '##te', ':']\n",
      "[101, 1292, 185, 131, 102]\n",
      "[CLS] Date : [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for wd in words[:10]:\n",
    "    print(tokenizer.tokenize(wd))\n",
    "    print(tokenizer.encode(wd))\n",
    "    print(tokenizer.decode(tokenizer.encode(wd)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bos_token',\n",
       " 'eos_token',\n",
       " 'unk_token',\n",
       " 'sep_token',\n",
       " 'pad_token',\n",
       " 'cls_token',\n",
       " 'mask_token',\n",
       " 'additional_special_tokens']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.SPECIAL_TOKENS_ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476, 2097, 5131]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Altempt\")[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{101: [0, 0, 0, 0], 102: [0, 0, 0, 0], 0: [0, 0, 0, 0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special2bbox = {\n",
    "    tokenizer.cls_token_id : lmv3_tok.cls_token_box,\n",
    "    tokenizer.sep_token_id : lmv3_tok.sep_token_box,\n",
    "    tokenizer.pad_token_id : lmv3_tok.pad_token_box\n",
    "}\n",
    "\n",
    "special2bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_bbox(\n",
    "    words, \n",
    "    bboxs, \n",
    "    tokenizer,\n",
    "    labels = [], \n",
    "    max_length = 512):\n",
    "    assert len(words) == len(bboxs)\n",
    "\n",
    "    input_ids = []\n",
    "    tokenized_boxes = []\n",
    "    tokenized_labels = []\n",
    "\n",
    "    if labels == []:\n",
    "        for wd, box in zip(words, bboxs):\n",
    "            tokenized_word_with_cls_sep = tokenizer.encode(wd)\n",
    "            tokenized_word = tokenized_word_with_cls_sep[1:-1]\n",
    "            tokenized_box = [box for k in tokenized_word]\n",
    "\n",
    "            input_ids += tokenized_word\n",
    "            tokenized_boxes += tokenized_box\n",
    "        \n",
    "        if len(input_ids) >= max_length - 2:\n",
    "            input_ids = [tokenizer.cls_token_id] + input_ids[:max_length - 2] + [tokenizer.sep_token_id]\n",
    "            tokenized_boxes = [special2bbox[tokenizer.cls_token_id]] + \\\n",
    "                    tokenized_boxes[:max_length - 2] + \\\n",
    "                    [special2bbox[tokenizer.sep_token_id]]\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"bbox\": tokenized_boxes}\n",
    "    else:\n",
    "        assert len(labels) == len(words)\n",
    "\n",
    "        for wd, box, label in zip(words, bboxs, labels):\n",
    "            tokenized_word_with_cls_sep = tokenizer.encode(wd)\n",
    "            tokenized_word = tokenized_word_with_cls_sep[1:-1]\n",
    "            tokenized_box = [box for k in tokenized_word]\n",
    "            tokenized_label = [-100 for k in tokenized_word]\n",
    "            tokenized_label[0] = label\n",
    "\n",
    "            input_ids += tokenized_word\n",
    "            tokenized_boxes += tokenized_box\n",
    "            tokenized_labels += tokenized_label\n",
    "        \n",
    "        if len(input_ids) >= max_length - 2:\n",
    "            input_ids = [tokenizer.cls_token_id] + input_ids[:max_length - 2] + [tokenizer.sep_token_id]\n",
    "            tokenized_boxes = [special2bbox[tokenizer.cls_token_id]] + \\\n",
    "                    tokenized_boxes[:max_length - 2] + \\\n",
    "                    [special2bbox[tokenizer.sep_token_id]]\n",
    "            tokenized_labels = [-100] + tokenized_labels[:max_length - 2] + [-100]\n",
    "        \n",
    "\n",
    "        return {\"input_ids\": input_ids, \"bbox\": tokenized_boxes, \"labels\": tokenized_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<unk>', '<pad>', '<mask>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmv3_tok.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenize_with_bbox(words, boxes, tokenizer, max_length=100)['input_ids']), len(tokenize_with_bbox(words, boxes, tokenizer, max_length=100)['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': 0, 'bbox': [0, 0, 0, 0], 'label': -100}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_obj = {\n",
    "    \"input_ids\": tokenizer.pad_token_id,\n",
    "    \"bbox\": special2bbox[tokenizer.pad_token_id],\n",
    "    \"label\": -100\n",
    "}\n",
    "\n",
    "pad_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os tokens CLS, SEP e PAD terão [0,0,0,0] como bounding box\n",
    "\n",
    "O token de mask mantém a mesma bounding box\n",
    "\n",
    "O token unk mantém a mesma bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['[MASK]'], [101, 103, 102], '[CLS] [MASK] [SEP]')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"[MASK]\"), tokenizer.encode(\"[MASK]\"), tokenizer.decode(tokenizer.encode(\"[MASK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokenized(tokenized_dict, max_length):\n",
    "\n",
    "    paded_dict = tokenized_dict.copy()\n",
    "\n",
    "    for key, val in tokenized_dict.items():\n",
    "        if len(val) >= max_length:\n",
    "            return tokenized_dict\n",
    "        \n",
    "        paded_dict[key] = val + [pad_obj[key] for k in range(max_length - len(val))]\n",
    "    \n",
    "    return paded_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9109,\n",
       " 22284,\n",
       " 438,\n",
       " 416,\n",
       " 1814,\n",
       " 156,\n",
       " 4267,\n",
       " 12893,\n",
       " 12230,\n",
       " 352,\n",
       " 11760,\n",
       " 4530,\n",
       " 143,\n",
       " 14883,\n",
       " 823,\n",
       " 6060,\n",
       " 22281,\n",
       " 438,\n",
       " 15460,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_tokenized(\n",
    "    tokenize_with_bbox(words, boxes, tokenizer, max_length=100),\n",
    "    max_length= 150\n",
    ")[\"input_ids\"][-70:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pad_tokenized(\n",
    "    tokenize_with_bbox(words, boxes, tokenizer, max_length=100),\n",
    "    max_length= 150\n",
    ")[\"bbox\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pad_tokenized(\n",
    "    tokenize_with_bbox(words, boxes, tokenizer, max_length=100),\n",
    "    max_length= 150\n",
    ")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "image = []\n",
    "boxes = []\n",
    "words = []\n",
    "word_labels = []\n",
    "\n",
    "for i in range(1):\n",
    "    image.append(read_image(a_img_path[i]))\n",
    "    text_boxes, shape = get_ocr_word_box_list(a_img_path[i])\n",
    "    words.append([k[\"text\"] for k in text_boxes])\n",
    "    boxes.append([normalize_bbox(k[\"bbox\"], shape[0], shape[1]) for k in text_boxes])\n",
    "    word_labels.append([len(word) for word in [k[\"text\"] for k in text_boxes]])\n",
    "\n",
    "\n",
    "processed = lmv3_processor(\n",
    "    image,\n",
    "    words,\n",
    "    boxes=boxes,\n",
    "    word_labels=word_labels,\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([1, 512]) 2\n",
      "attention_mask torch.Size([1, 512]) 2\n",
      "bbox torch.Size([1, 512, 4]) 3\n",
      "labels torch.Size([1, 512]) 2\n",
      "pixel_values torch.Size([1, 3, 224, 224]) 4\n"
     ]
    }
   ],
   "source": [
    "for k, v in processed.items():\n",
    "    print(k, v.shape, len(v.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_processor import BertimbauLayoutLMv3Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLprocessor = BertimbauLayoutLMv3Processor(\n",
    "    layoutlmv3_processor=lmv3_processor,\n",
    "    bertimbau_tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_BL = BLprocessor(\n",
    "    images=image,\n",
    "    words=words,\n",
    "    boxes=boxes,\n",
    "    word_labels=word_labels,\n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([1, 3, 224, 224]) 4\n",
      "input_ids torch.Size([1, 512]) 2\n",
      "bbox torch.Size([1, 512, 4]) 3\n",
      "labels torch.Size([1, 512]) 2\n",
      "attention_mask torch.Size([1, 512]) 2\n"
     ]
    }
   ],
   "source": [
    "for k, v in processed_BL.items():\n",
    "    print(k, v.shape, len(v.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([1, 3, 224, 224]) 4\n",
      "input_ids torch.Size([1, 512]) 2\n",
      "bbox torch.Size([1, 512, 4]) 3\n",
      "labels torch.Size([1, 512]) 2\n",
      "attention_mask torch.Size([1, 512]) 2\n"
     ]
    }
   ],
   "source": [
    "processed_BL_1 = BLprocessor(\n",
    "    images=image[0],\n",
    "    words=words[0],\n",
    "    boxes=boxes[0],\n",
    "    word_labels=word_labels[0],\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "for k, v in processed_BL_1.items():\n",
    "    print(k, v.shape, len(v.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('all': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ac26dd6dfbaca8db3a81442d3498aaba3accde3927255076a0ada8b4751f69b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
